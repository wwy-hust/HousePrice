#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›½å®¶ç»Ÿè®¡å±€æˆ¿ä»·æ•°æ®URLæ›´æ–°è„šæœ¬
è‡ªåŠ¨æ£€æŸ¥å¹¶æ›´æ–°HousePriceURL.csvä¸­çš„70ä¸ªå¤§ä¸­åŸå¸‚å•†å“ä½å®…é”€å”®ä»·æ ¼å˜åŠ¨æƒ…å†µæ•°æ®
"""

import requests
from bs4 import BeautifulSoup
import csv
import re
from datetime import datetime
import time
import sys
import subprocess
from urllib.parse import urljoin, urlparse

class HousePriceURLUpdater:
    def __init__(self, csv_file_path="HousePriceURL.csv"):
        self.csv_file_path = csv_file_path
        self.base_url = "https://www.stats.gov.cn"
        self.data_url = "https://www.stats.gov.cn/sj/zxfb/"
        self.existing_urls = set()
        self.existing_data = []
        
        # è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def load_existing_data(self):
        """è¯»å–ç°æœ‰çš„CSVæ–‡ä»¶æ•°æ®"""
        try:
            with open(self.csv_file_path, 'r', encoding='utf-8') as file:
                reader = csv.reader(file)
                rows = list(reader)
                
                if rows:
                    # è·³è¿‡æ ‡é¢˜è¡Œ
                    self.existing_data = rows
                    for row in rows[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                        if len(row) >= 2:
                            self.existing_urls.add(row[1])  # URLåˆ—
                            
            print(f"å·²è¯»å–ç°æœ‰æ•°æ®: {len(self.existing_urls)} æ¡è®°å½•")
            
        except FileNotFoundError:
            print("CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
            self.existing_data = [["æ ‡é¢˜", "æ ‡é¢˜é“¾æ¥", "æ—¶é—´"]]
        except Exception as e:
            print(f"è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            sys.exit(1)
    
    def get_page_content(self, url, max_retries=3):
        """è·å–ç½‘é¡µå†…å®¹ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(max_retries):
            try:
                print(f"æ­£åœ¨è®¿é—®: {url} (å°è¯• {attempt + 1}/{max_retries})")
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
                
            except requests.RequestException as e:
                print(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                else:
                    print(f"æ— æ³•è®¿é—® {url}")
                    return None
    
    def parse_date_from_text(self, text):
        """ä»æ–‡æœ¬ä¸­è§£ææ—¥æœŸ"""
        # åŒ¹é…å„ç§æ—¥æœŸæ ¼å¼
        date_patterns = [
            r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2025-08-15
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2025å¹´8æœˆ15æ—¥
            r'(\d{4})/(\d{1,2})/(\d{1,2})',  # 2025/8/15
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                year, month, day = match.groups()
                return f"{year}/{int(month)}/{int(day)}"
        
        return None
    
    def extract_house_price_data_from_page(self, html_content):
        """ä»å•é¡µHTMLå†…å®¹ä¸­æå–70ä¸ªå¤§ä¸­åŸå¸‚å•†å“ä½å®…é”€å”®ä»·æ ¼å˜åŠ¨æƒ…å†µçš„æ•°æ®"""
        soup = BeautifulSoup(html_content, 'html.parser')
        page_records = []
        
        # æŸ¥æ‰¾åŒ…å«æˆ¿ä»·æ•°æ®çš„é“¾æ¥
        links = soup.find_all('a', href=True)
        
        for link in links:
            link_text = link.get_text(strip=True)
            href = link.get('href')
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
            if "70ä¸ªå¤§ä¸­åŸå¸‚å•†å“ä½å®…é”€å”®ä»·æ ¼å˜åŠ¨æƒ…å†µ" in link_text:
                # æ„å»ºå®Œæ•´URL - å¤„ç†ç›¸å¯¹è·¯å¾„
                if href.startswith('http'):
                    full_url = href
                elif href.startswith('./'):
                    # å¤„ç† ./202508/t20250815_1960781.html æ ¼å¼
                    full_url = urljoin(self.data_url, href[2:])  # å»æ‰ ./
                else:
                    full_url = urljoin(self.base_url, href)
                
                # ä»çˆ¶å…ƒç´ çš„æ–‡æœ¬ä¸­æå–æ—¥æœŸ
                date_str = None
                parent = link.parent
                if parent:
                    parent_text = parent.get_text()
                    # æŸ¥æ‰¾æ—¥æœŸæ¨¡å¼ 2025-08-15
                    date_match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', parent_text)
                    if date_match:
                        year, month, day = date_match.groups()
                        date_str = f"{year}/{int(month)}/{int(day)}"
                
                # å¦‚æœæ²¡æ‰¾åˆ°æ—¥æœŸï¼Œä»é“¾æ¥æ–‡æœ¬ä¸­æ¨æ–­
                if not date_str:
                    date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ', link_text)
                    if date_match:
                        year, month = date_match.groups()
                        # æ ¹æ®å†å²æ•°æ®ï¼Œæˆ¿ä»·æ•°æ®é€šå¸¸åœ¨æ¬¡æœˆä¸­æ—¬å‘å¸ƒ
                        if int(month) == 12:
                            date_str = f"{int(year)+1}/1/15"
                        else:
                            date_str = f"{year}/{int(month)+1}/15"
                
                # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºfallback
                if not date_str:
                    date_str = datetime.now().strftime("%Y/%m/%d")
                
                record = {
                    'title': link_text,
                    'url': full_url,
                    'date': date_str
                }
                page_records.append(record)
        
        return page_records
    
    def extract_house_price_data(self):
        """ä»å¤šé¡µä¸­æå–æˆ¿ä»·æ•°æ®ï¼Œç›´åˆ°é‡åˆ°å·²å­˜åœ¨çš„æ¡ç›®"""
        all_new_records = []
        seen_urls = set()  # ç”¨äºå»é‡
        page_num = 1
        max_pages = 10  # æœ€å¤šæ£€æŸ¥10é¡µï¼Œé¿å…æ— é™å¾ªç¯
        
        while page_num <= max_pages:
            # æ„å»ºé¡µé¢URL
            if page_num == 1:
                page_url = self.data_url
            else:
                # å°è¯•ä¸åŒçš„åˆ†é¡µURLæ¨¡å¼
                page_patterns = [
                    f"{self.data_url}index_{page_num}.html",
                    f"{self.data_url}?page={page_num}",
                    f"{self.base_url}/sj/zxfb/index_{page_num}.html"
                ]
                
                page_url = None
                for pattern in page_patterns:
                    test_content = self.get_page_content(pattern)
                    if test_content and len(test_content) > 1000:  # ç®€å•æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰æ•ˆ
                        page_url = pattern
                        break
                
                if not page_url:
                    print(f"æ— æ³•è®¿é—®ç¬¬{page_num}é¡µï¼Œåœæ­¢ç¿»é¡µ")
                    break
            
            print(f"æ­£åœ¨æ£€æŸ¥ç¬¬{page_num}é¡µ: {page_url}")
            
            # è·å–é¡µé¢å†…å®¹
            html_content = self.get_page_content(page_url)
            if not html_content:
                print(f"æ— æ³•è·å–ç¬¬{page_num}é¡µå†…å®¹ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # æå–å½“å‰é¡µçš„æˆ¿ä»·æ•°æ®
            page_records = self.extract_house_price_data_from_page(html_content)
            
            if not page_records:
                print(f"ç¬¬{page_num}é¡µæœªæ‰¾åˆ°æˆ¿ä»·æ•°æ®ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            print(f"ç¬¬{page_num}é¡µæ‰¾åˆ° {len(page_records)} ä¸ªæˆ¿ä»·æ•°æ®æ¡ç›®")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®ï¼Œä»¥åŠæ˜¯å¦é‡åˆ°å·²å­˜åœ¨çš„æ¡ç›®
            found_existing = False
            page_new_records = []
            
            for record in page_records:
                # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
                if record['url'] in self.existing_urls:
                    print(f"é‡åˆ°å·²å­˜åœ¨æ¡ç›®ï¼Œåœæ­¢ç¿»é¡µ: {record['title']}")
                    found_existing = True
                    break
                
                # æ£€æŸ¥æ˜¯å¦åœ¨å½“å‰æ‰¹æ¬¡ä¸­é‡å¤
                if record['url'] not in seen_urls:
                    seen_urls.add(record['url'])
                    page_new_records.append(record)
                    print(f"å‘ç°æ–°è®°å½•: {record['title']} -> {record['url']}")
            
            # æ·»åŠ æ–°è®°å½•
            all_new_records.extend(page_new_records)
            
            # å¦‚æœé‡åˆ°å·²å­˜åœ¨çš„æ¡ç›®ï¼Œåœæ­¢ç¿»é¡µ
            if found_existing:
                break
            
            page_num += 1
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        print(f"ç¿»é¡µå®Œæˆï¼Œå…±æ£€æŸ¥äº† {page_num} é¡µï¼Œæ‰¾åˆ° {len(all_new_records)} ä¸ªæ–°è®°å½•")
        return all_new_records
    
    def extract_house_price_data_enhanced(self):
        """å¢å¼ºç‰ˆæ•°æ®æå–ï¼Œä¸“é—¨å¤„ç†å›½å®¶ç»Ÿè®¡å±€ç½‘ç«™ç»“æ„"""
        all_new_records = []
        seen_urls = set()
        
        print("ä½¿ç”¨å¢å¼ºç‰ˆæ•°æ®æå–æ–¹æ³•...")
        
        # é¦–å…ˆå°è¯•ä¸»é¡µé¢
        html_content = self.get_page_content(self.data_url)
        if html_content:
            print("æ­£åœ¨åˆ†æä¸»é¡µé¢ç»“æ„...")
            
            # ä½¿ç”¨æ›´çµæ´»çš„è§£ææ–¹æ³•
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æˆ¿ä»·å…³é”®è¯çš„é“¾æ¥
            all_links = soup.find_all('a', href=True)
            print(f"é¡µé¢ä¸­æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
            
            for link in all_links:
                link_text = link.get_text(strip=True)
                href = link.get('href')
                
                # æ›´å®½æ¾çš„åŒ¹é…æ¡ä»¶
                if any(keyword in link_text for keyword in [
                    "70ä¸ªå¤§ä¸­åŸå¸‚", "å•†å“ä½å®…é”€å”®ä»·æ ¼", "æˆ¿ä»·å˜åŠ¨", "ä½å®…é”€å”®ä»·æ ¼"
                ]):
                    print(f"æ‰¾åˆ°ç›¸å…³é“¾æ¥: {link_text}")
                    
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('http'):
                        full_url = href
                    elif href.startswith('./'):
                        full_url = urljoin(self.data_url, href[2:])
                    else:
                        full_url = urljoin(self.base_url, href)
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    if full_url in self.existing_urls:
                        print(f"URLå·²å­˜åœ¨ï¼Œè·³è¿‡: {link_text}")
                        continue
                    
                    if full_url in seen_urls:
                        continue
                    
                    seen_urls.add(full_url)
                    
                    # æå–æ—¥æœŸ
                    date_str = self.extract_date_from_link_context(link, soup)
                    
                    record = {
                        'title': link_text,
                        'url': full_url,
                        'date': date_str
                    }
                    all_new_records.append(record)
                    print(f"æ·»åŠ æ–°è®°å½•: {link_text} -> {full_url}")
        
        print(f"å¢å¼ºç‰ˆæå–å®Œæˆï¼Œæ‰¾åˆ° {len(all_new_records)} ä¸ªæ–°è®°å½•")
        return all_new_records
    
    def extract_date_from_link_context(self, link, soup):
        """ä»é“¾æ¥ä¸Šä¸‹æ–‡æå–æ—¥æœŸ"""
        # å°è¯•ä»çˆ¶å…ƒç´ è·å–æ—¥æœŸ
        parent = link.parent
        while parent and parent.name != 'body':
            parent_text = parent.get_text()
            
            # æŸ¥æ‰¾æ—¥æœŸæ¨¡å¼
            date_patterns = [
                r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2025-10-20
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2025å¹´10æœˆ20æ—¥
                r'(\d{4})/(\d{1,2})/(\d{1,2})',  # 2025/10/20
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, parent_text)
                if match:
                    year, month, day = match.groups()
                    return f"{year}/{int(month)}/{int(day)}"
            
            parent = parent.parent
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä»é“¾æ¥æ–‡æœ¬æ¨æ–­
        date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ', link.get_text())
        if date_match:
            year, month = date_match.groups()
            # æ ¹æ®å†å²æ•°æ®ï¼Œæˆ¿ä»·æ•°æ®é€šå¸¸åœ¨æ¬¡æœˆä¸­æ—¬å‘å¸ƒ
            if int(month) == 12:
                return f"{int(year)+1}/1/15"
            else:
                return f"{year}/{int(month)+1}/15"
        
        # é»˜è®¤ä½¿ç”¨å½“å‰æ—¥æœŸ
        return datetime.now().strftime("%Y/%m/%d")
    
    def get_accurate_date_from_detail_page(self, url):
        """ä»è¯¦æƒ…é¡µé¢è·å–å‡†ç¡®çš„å‘å¸ƒæ—¥æœŸ"""
        try:
            content = self.get_page_content(url)
            if not content:
                return None
                
            soup = BeautifulSoup(content, 'html.parser')
            
            # å¯»æ‰¾å‘å¸ƒæ—¥æœŸçš„å¸¸è§ä½ç½®
            date_selectors = [
                '.artical-info',
                '.article-info', 
                '.publish-time',
                '.time',
                '.date'
            ]
            
            for selector in date_selectors:
                date_element = soup.select_one(selector)
                if date_element:
                    date_text = date_element.get_text()
                    parsed_date = self.parse_date_from_text(date_text)
                    if parsed_date:
                        return parsed_date
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåœ¨æ•´ä¸ªé¡µé¢ä¸­æœç´¢æ—¥æœŸæ¨¡å¼
            page_text = soup.get_text()
            parsed_date = self.parse_date_from_text(page_text)
            if parsed_date:
                return parsed_date
                
        except Exception as e:
            print(f"è·å–è¯¦æƒ…é¡µé¢æ—¥æœŸæ—¶å‡ºé”™: {e}")
        
        return None
    
    def update_csv_file(self, new_records):
        """æ›´æ–°CSVæ–‡ä»¶"""
        if not new_records:
            print("æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ·»åŠ ")
            return
        
        # ä¸ºæ–°è®°å½•è·å–æ›´å‡†ç¡®çš„æ—¥æœŸ
        for record in new_records:
            print(f"è·å–è¯¦ç»†æ—¥æœŸ: {record['title']}")
            accurate_date = self.get_accurate_date_from_detail_page(record['url'])
            if accurate_date:
                record['date'] = accurate_date
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
        new_records.sort(key=lambda x: datetime.strptime(x['date'], "%Y/%m/%d"), reverse=True)
        
        # å‡†å¤‡å†™å…¥CSVçš„æ•°æ®
        all_rows = []
        
        # æ·»åŠ æ ‡é¢˜è¡Œ
        all_rows.append(["æ ‡é¢˜", "æ ‡é¢˜é“¾æ¥", "æ—¶é—´"])
        
        # æ·»åŠ æ–°è®°å½•
        for record in new_records:
            all_rows.append([record['title'], record['url'], record['date']])
        
        # æ·»åŠ ç°æœ‰è®°å½•ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰
        if len(self.existing_data) > 1:
            all_rows.extend(self.existing_data[1:])
        
        # å†™å…¥CSVæ–‡ä»¶
        try:
            with open(self.csv_file_path, 'w', encoding='utf-8', newline='') as file:
                writer = csv.writer(file)
                writer.writerows(all_rows)
            
            print(f"æˆåŠŸæ›´æ–°CSVæ–‡ä»¶ï¼Œæ·»åŠ äº† {len(new_records)} æ¡æ–°è®°å½•")
            for record in new_records:
                print(f"  - {record['title']} ({record['date']})")
                
        except Exception as e:
            print(f"å†™å…¥CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def collect_new_data(self, new_records):
        """ä¸ºæ–°è®°å½•é‡‡é›†æ•°æ®"""
        if not new_records:
            return True
            
        print("\n" + "=" * 50)
        print("å¼€å§‹é‡‡é›†æ–°æ•°æ®...")
        print("=" * 50)
        
        try:
            # å¯¼å…¥æ•°æ®é‡‡é›†å™¨
            from data_collector import HousePriceDataCollector
            collector = HousePriceDataCollector()
            
            success_count = 0
            total_count = len(new_records)
            
            for i, record in enumerate(new_records, 1):
                print(f"\næ­£åœ¨é‡‡é›†ç¬¬ {i}/{total_count} ä¸ªæ•°æ®:")
                print(f"æ ‡é¢˜: {record['title']}")
                print(f"URL: {record['url']}")
                print(f"æ—¥æœŸ: {record['date']}")
                
                # é‡‡é›†å•ä¸ªURLçš„æ•°æ®
                success = collector.collect_single_url_data(
                    record['url'], 
                    record['title'], 
                    record['date']
                )
                
                if success:
                    success_count += 1
                    print(f"âœ… é‡‡é›†æˆåŠŸ ({success_count}/{total_count})")
                else:
                    print(f"âŒ é‡‡é›†å¤±è´¥")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if i < total_count:
                    time.sleep(2)
            
            print(f"\næ•°æ®é‡‡é›†å®Œæˆ: æˆåŠŸ {success_count}/{total_count}")
            return success_count > 0
            
        except ImportError as e:
            print(f"æ— æ³•å¯¼å…¥æ•°æ®é‡‡é›†å™¨: {e}")
            return False
        except Exception as e:
            print(f"æ•°æ®é‡‡é›†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return False
    
    def process_data_to_json(self):
        """å¤„ç†XMLæ•°æ®ä¸ºJSONæ ¼å¼"""
        print("\n" + "=" * 50)
        print("å¼€å§‹å¤„ç†æ•°æ®ä¸ºJSONæ ¼å¼...")
        print("=" * 50)
        
        try:
            # å¯¼å…¥æ‰¹é‡å¤„ç†å™¨
            from batch_process_all_cities import BatchProcessor
            
            # åˆ›å»ºæ‰¹é‡å¤„ç†å™¨å¹¶å¼€å§‹å¤„ç†
            processor = BatchProcessor()
            processor.process_all()
            
            print("âœ… æ•°æ®å¤„ç†å®Œæˆï¼JSONæ–‡ä»¶å·²æ›´æ–°ã€‚")
            return True
            
        except ImportError as e:
            print(f"æ— æ³•å¯¼å…¥æ‰¹é‡å¤„ç†å™¨: {e}")
            return False
        except Exception as e:
            print(f"æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return False
    
    def run(self):
        """è¿è¡Œæ›´æ–°ç¨‹åº"""
        print("å¼€å§‹æ›´æ–°æˆ¿ä»·æ•°æ®URL...")
        print("=" * 50)
        
        # 1. è¯»å–ç°æœ‰æ•°æ®
        self.load_existing_data()
        
        # 2. é¦–å…ˆå°è¯•å¢å¼ºç‰ˆæ•°æ®æå–
        print("å°è¯•å¢å¼ºç‰ˆæ•°æ®æå–...")
        new_records = self.extract_house_price_data_enhanced()
        
        # 3. å¦‚æœå¢å¼ºç‰ˆæ²¡æœ‰æ‰¾åˆ°æ–°æ•°æ®ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
        if not new_records:
            print("å¢å¼ºç‰ˆæœªæ‰¾åˆ°æ–°æ•°æ®ï¼Œå°è¯•åŸå§‹ç¿»é¡µæ–¹æ³•...")
            new_records = self.extract_house_price_data()
        
        # 4. æ›´æ–°CSVæ–‡ä»¶
        self.update_csv_file(new_records)
        
        # 4. å¦‚æœæœ‰æ–°è®°å½•ï¼Œåˆ™è¿›è¡Œæ•°æ®é‡‡é›†å’Œå¤„ç†
        if new_records:
            print(f"\nå‘ç° {len(new_records)} ä¸ªæ–°è®°å½•ï¼Œå¼€å§‹è‡ªåŠ¨å¤„ç†...")
            
            # 4.1 é‡‡é›†æ–°æ•°æ®
            collect_success = self.collect_new_data(new_records)
            
            if collect_success:
                # 4.2 å¤„ç†æ•°æ®ä¸ºJSONæ ¼å¼
                process_success = self.process_data_to_json()
                
                if process_success:
                    print("\nğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸï¼")
                    print("- âœ… URLæ›´æ–°å®Œæˆ")
                    print("- âœ… æ•°æ®é‡‡é›†å®Œæˆ") 
                    print("- âœ… JSONæ–‡ä»¶æ›´æ–°å®Œæˆ")
                else:
                    print("\nâš ï¸  éƒ¨åˆ†æµç¨‹å®Œæˆ:")
                    print("- âœ… URLæ›´æ–°å®Œæˆ")
                    print("- âœ… æ•°æ®é‡‡é›†å®Œæˆ")
                    print("- âŒ JSONå¤„ç†å¤±è´¥")
            else:
                print("\nâš ï¸  æ•°æ®é‡‡é›†å¤±è´¥ï¼Œè·³è¿‡JSONå¤„ç†")
        else:
            print("\nâœ… æ— æ–°æ•°æ®ï¼Œæµç¨‹å®Œæˆ")
        
        print("=" * 50)
        print("æ›´æ–°å®Œæˆ!")

def main():
    updater = HousePriceURLUpdater()
    updater.run()

if __name__ == "__main__":
    main()
