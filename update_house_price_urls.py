#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›½å®¶ç»Ÿè®¡å±€æˆ¿ä»·æ•°æ®URLæ›´æ–°è„šæœ¬
è‡ªåŠ¨æ£€æŸ¥å¹¶æ›´æ–°HousePriceURL.csvä¸­çš„70ä¸ªå¤§ä¸­åŸå¸‚å•†å“ä½å®…é”€å”®ä»·æ ¼å˜åŠ¨æƒ…å†µæ•°æ®
"""

import requests
from bs4 import BeautifulSoup
import csv
import re
from datetime import datetime
import time
import sys
import subprocess
from urllib.parse import urljoin, urlparse

class HousePriceURLUpdater:
    def __init__(self, csv_file_path="HousePriceURL.csv"):
        self.csv_file_path = csv_file_path
        self.base_url = "https://www.stats.gov.cn"
        self.data_url = "https://www.stats.gov.cn/sj/zxfb/"
        self.existing_urls = set()
        self.existing_data = []
        self.latest_url = None  # CSVæ–‡ä»¶ä¸­ç¬¬ä¸€æ¡è®°å½•ï¼ˆæœ€æ–°è®°å½•ï¼‰çš„URL
        
        # è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def load_existing_data(self):
        """è¯»å–ç°æœ‰çš„CSVæ–‡ä»¶æ•°æ®"""
        try:
            with open(self.csv_file_path, 'r', encoding='utf-8') as file:
                reader = csv.reader(file)
                rows = list(reader)
                
                if rows:
                    # è·³è¿‡æ ‡é¢˜è¡Œ
                    self.existing_data = rows
                    for row in rows[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                        if len(row) >= 2:
                            self.existing_urls.add(row[1])  # URLåˆ—
                    
                    # è·å–ç¬¬ä¸€æ¡è®°å½•ï¼ˆæœ€æ–°è®°å½•ï¼‰çš„URLï¼Œä½œä¸ºåœæ­¢ç¿»é¡µçš„æ ‡è®°
                    if len(rows) > 1 and len(rows[1]) >= 2:
                        self.latest_url = rows[1][1]  # ç¬¬ä¸€æ¡æ•°æ®è¡Œçš„URLåˆ—
                        print(f"æœ€æ–°è®°å½•URLï¼ˆåœæ­¢æ ‡è®°ï¼‰: {self.latest_url}")
                            
            print(f"å·²è¯»å–ç°æœ‰æ•°æ®: {len(self.existing_urls)} æ¡è®°å½•")
            
        except FileNotFoundError:
            print("CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
            self.existing_data = [["æ ‡é¢˜", "æ ‡é¢˜é“¾æ¥", "æ—¶é—´"]]
            self.latest_url = None
        except Exception as e:
            print(f"è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            sys.exit(1)
    
    def get_page_content(self, url, max_retries=3):
        """è·å–ç½‘é¡µå†…å®¹ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(max_retries):
            try:
                print(f"æ­£åœ¨è®¿é—®: {url} (å°è¯• {attempt + 1}/{max_retries})")
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
                
            except requests.RequestException as e:
                print(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                else:
                    print(f"æ— æ³•è®¿é—® {url}")
                    return None
    
    def parse_date_from_text(self, text):
        """ä»æ–‡æœ¬ä¸­è§£ææ—¥æœŸ"""
        # åŒ¹é…å„ç§æ—¥æœŸæ ¼å¼
        date_patterns = [
            r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2025-08-15
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2025å¹´8æœˆ15æ—¥
            r'(\d{4})/(\d{1,2})/(\d{1,2})',  # 2025/8/15
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                year, month, day = match.groups()
                return f"{year}/{int(month)}/{int(day)}"
        
        return None
    
    def extract_house_price_data_from_page(self, html_content):
        """ä»å•é¡µHTMLå†…å®¹ä¸­æå–70ä¸ªå¤§ä¸­åŸå¸‚å•†å“ä½å®…é”€å”®ä»·æ ¼å˜åŠ¨æƒ…å†µçš„æ•°æ®"""
        soup = BeautifulSoup(html_content, 'html.parser')
        page_records = []
        
        # æŸ¥æ‰¾åŒ…å«æˆ¿ä»·æ•°æ®çš„é“¾æ¥
        links = soup.find_all('a', href=True)
        
        for link in links:
            link_text = link.get_text(strip=True)
            href = link.get('href')
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
            if "70ä¸ªå¤§ä¸­åŸå¸‚å•†å“ä½å®…é”€å”®ä»·æ ¼å˜åŠ¨æƒ…å†µ" in link_text:
                # æ„å»ºå®Œæ•´URL - å¤„ç†ç›¸å¯¹è·¯å¾„
                if href.startswith('http'):
                    full_url = href
                elif href.startswith('./'):
                    # å¤„ç† ./202508/t20250815_1960781.html æ ¼å¼
                    full_url = urljoin(self.data_url, href[2:])  # å»æ‰ ./
                else:
                    full_url = urljoin(self.base_url, href)
                
                # ä»çˆ¶å…ƒç´ çš„æ–‡æœ¬ä¸­æå–æ—¥æœŸ
                date_str = None
                parent = link.parent
                if parent:
                    parent_text = parent.get_text()
                    # æŸ¥æ‰¾æ—¥æœŸæ¨¡å¼ 2025-08-15
                    date_match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', parent_text)
                    if date_match:
                        year, month, day = date_match.groups()
                        date_str = f"{year}/{int(month)}/{int(day)}"
                
                # å¦‚æœæ²¡æ‰¾åˆ°æ—¥æœŸï¼Œä»é“¾æ¥æ–‡æœ¬ä¸­æ¨æ–­
                if not date_str:
                    date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ', link_text)
                    if date_match:
                        year, month = date_match.groups()
                        # æ ¹æ®å†å²æ•°æ®ï¼Œæˆ¿ä»·æ•°æ®é€šå¸¸åœ¨æ¬¡æœˆä¸­æ—¬å‘å¸ƒ
                        if int(month) == 12:
                            date_str = f"{int(year)+1}/1/15"
                        else:
                            date_str = f"{year}/{int(month)+1}/15"
                
                # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºfallback
                if not date_str:
                    date_str = datetime.now().strftime("%Y/%m/%d")
                
                record = {
                    'title': link_text,
                    'url': full_url,
                    'date': date_str
                }
                page_records.append(record)
        
        return page_records
    
    def extract_house_price_data(self):
        """ä»å¤šé¡µä¸­æå–æˆ¿ä»·æ•°æ®ï¼Œç›´åˆ°é‡åˆ°å·²å­˜åœ¨çš„æ¡ç›®ï¼ˆæ”¹è¿›ç‰ˆï¼Œæ”¯æŒæ™ºèƒ½ç¿»é¡µï¼‰"""
        all_new_records = []
        seen_urls = set()  # ç”¨äºå»é‡
        visited_urls = set()  # è®°å½•å·²è®¿é—®çš„é¡µé¢URLï¼Œé¿å…é‡å¤è®¿é—®
        page_num = 1
        max_pages = 100  # å¢åŠ æœ€å¤§é¡µæ•°ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°æœ€æ–°è®°å½•ï¼ˆæœ€å¤šæ£€æŸ¥100é¡µï¼‰
        
        # ä»ç¬¬ä¸€é¡µå¼€å§‹ï¼ˆä½¿ç”¨ index.html æ ¼å¼ï¼‰
        current_url = f"{self.data_url}index.html"
        
        while current_url and page_num <= max_pages:
            # é¿å…é‡å¤è®¿é—®
            if current_url in visited_urls:
                print(f"é¡µé¢å·²è®¿é—®è¿‡ï¼Œè·³è¿‡: {current_url}")
                break
            
            visited_urls.add(current_url)
            print(f"\næ­£åœ¨æ£€æŸ¥ç¬¬ {page_num} é¡µ: {current_url}")
            
            # è·å–é¡µé¢å†…å®¹
            html_content = self.get_page_content(current_url)
            if not html_content:
                print(f"æ— æ³•è·å–ç¬¬ {page_num} é¡µå†…å®¹ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # è§£æé¡µé¢
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–å½“å‰é¡µçš„æˆ¿ä»·æ•°æ®
            page_records = self.extract_house_price_data_from_page(html_content)
            
            if not page_records:
                print(f"ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°æˆ¿ä»·æ•°æ®ï¼Œå°è¯•æŸ¥æ‰¾ä¸‹ä¸€é¡µ...")
                # å³ä½¿æ²¡æ‰¾åˆ°æ•°æ®ï¼Œä¹Ÿå°è¯•æŸ¥æ‰¾ä¸‹ä¸€é¡µï¼ˆå¯èƒ½æ•°æ®åœ¨åé¢çš„é¡µé¢ï¼‰
                next_url = self.find_next_page_url(soup, current_url)
                if next_url:
                    current_url = next_url
                    page_num += 1
                    time.sleep(1)
                    continue
                else:
                    print(f"æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œåœæ­¢ç¿»é¡µ")
                    break
            
            print(f"ç¬¬ {page_num} é¡µæ‰¾åˆ° {len(page_records)} ä¸ªæˆ¿ä»·æ•°æ®æ¡ç›®")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®ï¼Œä»¥åŠæ˜¯å¦é‡åˆ°CSVä¸­çš„ç¬¬ä¸€æ¡è®°å½•ï¼ˆæœ€æ–°è®°å½•ï¼‰
            found_latest_record = False  # æ˜¯å¦é‡åˆ°CSVä¸­çš„ç¬¬ä¸€æ¡è®°å½•ï¼ˆæœ€æ–°è®°å½•ï¼‰
            page_new_records = []
            
            for record in page_records:
                # æ£€æŸ¥æ˜¯å¦é‡åˆ°CSVä¸­çš„ç¬¬ä¸€æ¡è®°å½•ï¼ˆæœ€æ–°è®°å½•ï¼‰
                if self.latest_url and record['url'] == self.latest_url:
                    print(f"é‡åˆ°CSVä¸­çš„æœ€æ–°è®°å½•ï¼Œåœæ­¢ç¿»é¡µ: {record['title']}")
                    found_latest_record = True
                    break
                
                # å¦‚æœURLå·²å­˜åœ¨ä½†ä¸æ˜¯æœ€æ–°è®°å½•ï¼Œè·³è¿‡ä½†ç»§ç»­ç¿»é¡µ
                if record['url'] in self.existing_urls:
                    print(f"é‡åˆ°å·²å­˜åœ¨æ¡ç›®ï¼ˆéæœ€æ–°ï¼‰ï¼Œè·³è¿‡ä½†ç»§ç»­ç¿»é¡µ: {record['title']}")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åœ¨å½“å‰æ‰¹æ¬¡ä¸­é‡å¤
                if record['url'] not in seen_urls:
                    seen_urls.add(record['url'])
                    page_new_records.append(record)
                    print(f"å‘ç°æ–°è®°å½•: {record['title']} -> {record['url']}")
            
            # æ·»åŠ æ–°è®°å½•
            all_new_records.extend(page_new_records)
            print(f"ç¬¬ {page_num} é¡µæ‰¾åˆ° {len(page_new_records)} ä¸ªæ–°è®°å½•")
            
            # å¦‚æœé‡åˆ°CSVä¸­çš„ç¬¬ä¸€æ¡è®°å½•ï¼ˆæœ€æ–°è®°å½•ï¼‰ï¼Œåœæ­¢ç¿»é¡µ
            if found_latest_record:
                break
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µURL
            next_url = self.find_next_page_url(soup, current_url)
            if next_url:
                current_url = next_url
                page_num += 1
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œå°è¯•ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•æ„å»ºURL
                # åŸºäºå½“å‰URLæ„å»ºä¸‹ä¸€é¡µ
                if current_url.endswith('index.html'):
                    # ä» index.html åˆ° index_1.html
                    next_url = current_url.replace('index.html', 'index_1.html')
                    page_patterns = [next_url]
                elif 'index_' in current_url:
                    # å¦‚æœURLåŒ…å« index_ï¼Œæå–å½“å‰é¡µç å¹¶åŠ 1
                    match = re.search(r'index_(\d+)\.html', current_url)
                    if match:
                        current_page = int(match.group(1))
                        next_page = current_page + 1
                        next_url = re.sub(r'index_\d+\.html', f'index_{next_page}.html', current_url)
                        page_patterns = [next_url]
                    else:
                        page_patterns = [f"{self.data_url}index_1.html"]
                else:
                    # å¦‚æœURLä¸åŒ…å« index_ï¼Œæ„å»º index_1.htmlï¼ˆç¬¬äºŒé¡µï¼‰
                    page_patterns = [
                        f"{self.data_url}index_1.html",
                        f"{self.data_url}?page=2",
                        f"{self.base_url}/sj/zxfb/index_1.html"
                    ]
                
                next_url = None
                for pattern in page_patterns:
                    # ç®€å•æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆï¼ˆä¸å®é™…è®¿é—®ï¼Œåªæ£€æŸ¥æ ¼å¼ï¼‰
                    if pattern not in visited_urls:
                        next_url = pattern
                        break
                
                if next_url:
                    current_url = next_url
                    page_num += 1
                    time.sleep(1)
                else:
                    print(f"æ— æ³•æ‰¾åˆ°æˆ–æ„å»ºä¸‹ä¸€é¡µURLï¼Œåœæ­¢ç¿»é¡µ")
                    break
        
        print(f"\nç¿»é¡µå®Œæˆï¼Œå…±æ£€æŸ¥äº† {page_num} é¡µï¼Œæ‰¾åˆ° {len(all_new_records)} ä¸ªæ–°è®°å½•")
        return all_new_records
    
    def find_next_page_url(self, soup, current_url):
        """ä»é¡µé¢ä¸­æŸ¥æ‰¾ä¸‹ä¸€é¡µçš„URL"""
        # æŸ¥æ‰¾å¸¸è§çš„åˆ†é¡µé“¾æ¥æ¨¡å¼
        next_page_keywords = ['ä¸‹ä¸€é¡µ', 'ä¸‹é¡µ', 'next', '>', 'Â»']
        
        # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«"ä¸‹ä¸€é¡µ"æ–‡æœ¬çš„é“¾æ¥
        for keyword in next_page_keywords:
            next_links = soup.find_all('a', href=True, string=re.compile(keyword, re.I))
            for link in next_links:
                href = link.get('href')
                if href:
                    if href.startswith('http'):
                        return href
                    elif href.startswith('./'):
                        return urljoin(self.data_url, href[2:])
                    else:
                        return urljoin(self.base_url, href)
        
        # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å«"ä¸‹ä¸€é¡µ"æ–‡æœ¬çš„çˆ¶å…ƒç´ ä¸­çš„é“¾æ¥
        for keyword in next_page_keywords:
            elements = soup.find_all(string=re.compile(keyword, re.I))
            for elem in elements:
                parent = elem.parent
                while parent:
                    if parent.name == 'a' and parent.get('href'):
                        href = parent.get('href')
                        if href.startswith('http'):
                            return href
                        elif href.startswith('./'):
                            return urljoin(self.data_url, href[2:])
                        else:
                            return urljoin(self.base_url, href)
                    parent = parent.parent
        
        # æ–¹æ³•3: å¤„ç†ç‰¹æ®Šçš„åˆ†é¡µæ¨¡å¼
        # å¦‚æœå½“å‰æ˜¯ index.htmlï¼Œä¸‹ä¸€é¡µæ˜¯ index_1.html
        if current_url.endswith('index.html') or current_url == f"{self.data_url}index.html":
            return f"{self.data_url}index_1.html"
        
        # å¦‚æœå½“å‰æ˜¯ index_N.htmlï¼Œä¸‹ä¸€é¡µæ˜¯ index_(N+1).html
        page_num_match = re.search(r'index_(\d+)\.html', current_url)
        if page_num_match:
            current_page = int(page_num_match.group(1))
            next_page = current_page + 1
            # å°è¯•æ„å»ºä¸‹ä¸€é¡µURL
            next_url = current_url.replace(f'index_{current_page}.html', f'index_{next_page}.html')
            return next_url
        
        # æ–¹æ³•4: æŸ¥æ‰¾æ‰€æœ‰æ•°å­—é“¾æ¥ï¼Œå°è¯•æ‰¾åˆ°ä¸‹ä¸€é¡µ
        page_links = soup.find_all('a', href=re.compile(r'index_\d+\.html'))
        if page_links:
            page_numbers = []
            for link in page_links:
                href = link.get('href', '')
                match = re.search(r'index_(\d+)\.html', href)
                if match:
                    page_numbers.append(int(match.group(1)))
            
            if page_numbers:
                # ä»å½“å‰URLæå–é¡µç 
                current_page = 0  # 0è¡¨ç¤ºindex.htmlï¼ˆç¬¬ä¸€é¡µï¼‰
                match = re.search(r'index_(\d+)\.html', current_url)
                if match:
                    current_page = int(match.group(1))
                elif current_url.endswith('index.html'):
                    current_page = 0  # index.html æ˜¯ç¬¬ä¸€é¡µ
                
                # æ‰¾åˆ°ä¸‹ä¸€é¡µ
                next_page = current_page + 1
                max_page = max(page_numbers)
                # å¦‚æœä¸‹ä¸€é¡µåœ¨é¡µé¢é“¾æ¥ä¸­ï¼Œæˆ–è€…ä¸‹ä¸€é¡µä¸è¶…è¿‡æœ€å¤§é¡µç ï¼ˆå…è®¸å°è¯•ï¼‰
                if next_page in page_numbers or (next_page <= max_page + 2):  # å…è®¸å°è¯•è¶…å‡º2é¡µ
                    # æ„å»ºä¸‹ä¸€é¡µURL
                    if current_url.endswith('/'):
                        if next_page == 1:
                            next_url = f"{current_url}index_1.html"
                        else:
                            next_url = f"{current_url}index_{next_page}.html"
                    elif 'index_' in current_url:
                        next_url = re.sub(r'index_\d+\.html', f'index_{next_page}.html', current_url)
                    elif current_url.endswith('index.html'):
                        # ä» index.html åˆ° index_1.html
                        next_url = current_url.replace('index.html', 'index_1.html')
                    else:
                        next_url = urljoin(current_url, f'index_{next_page}.html')
                    return next_url
        
        # æ–¹æ³•5: å¦‚æœå½“å‰URLæ˜¯ index.htmlï¼ˆç¬¬ä¸€é¡µï¼‰ï¼Œæ„å»º index_1.htmlï¼ˆç¬¬äºŒé¡µï¼‰
        if current_url.endswith('index.html') or current_url == f"{self.data_url}index.html":
            return f"{self.data_url}index_1.html"
        
        # æ–¹æ³•6: å¦‚æœå½“å‰URLæ˜¯ç¬¬ä¸€é¡µï¼ˆæ²¡æœ‰index_ï¼‰ï¼Œå°è¯•æ„å»ºç¬¬äºŒé¡µURL
        if not re.search(r'index_\d+\.html', current_url) and not current_url.endswith('index.html'):
            # å½“å‰URLæ˜¯ç¬¬ä¸€é¡µï¼Œå°è¯•æ„å»ºç¬¬äºŒé¡µ
            if current_url.endswith('/'):
                next_url = f"{current_url}index_1.html"
            else:
                # å¦‚æœURLæ˜¯ https://www.stats.gov.cn/sj/zxfb/ï¼Œæ„å»º index_1.html
                if current_url == self.data_url:
                    next_url = f"{self.data_url}index_1.html"
                else:
                    next_url = f"{current_url}/index_1.html"
            return next_url
        
        return None
    
    def extract_house_price_data_enhanced(self):
        """å¢å¼ºç‰ˆæ•°æ®æå–ï¼Œä¸“é—¨å¤„ç†å›½å®¶ç»Ÿè®¡å±€ç½‘ç«™ç»“æ„ï¼Œæ”¯æŒç¿»é¡µ"""
        all_new_records = []
        seen_urls = set()
        visited_urls = set()  # è®°å½•å·²è®¿é—®çš„é¡µé¢URLï¼Œé¿å…é‡å¤è®¿é—®
        
        print("ä½¿ç”¨å¢å¼ºç‰ˆæ•°æ®æå–æ–¹æ³•ï¼ˆæ”¯æŒç¿»é¡µï¼‰...")
        
        # ä»ç¬¬ä¸€é¡µå¼€å§‹ï¼ˆä½¿ç”¨ index.html æ ¼å¼ï¼‰
        current_url = f"{self.data_url}index.html"
        page_num = 1
        max_pages = 100  # å¢åŠ æœ€å¤§é¡µæ•°ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°æœ€æ–°è®°å½•ï¼ˆæœ€å¤šæ£€æŸ¥100é¡µï¼‰
        
        while current_url and page_num <= max_pages:
            # é¿å…é‡å¤è®¿é—®
            if current_url in visited_urls:
                print(f"é¡µé¢å·²è®¿é—®è¿‡ï¼Œè·³è¿‡: {current_url}")
                break
            
            visited_urls.add(current_url)
            print(f"\næ­£åœ¨æ£€æŸ¥ç¬¬ {page_num} é¡µ: {current_url}")
            
            # è·å–é¡µé¢å†…å®¹
            html_content = self.get_page_content(current_url)
            if not html_content:
                print(f"æ— æ³•è·å–ç¬¬ {page_num} é¡µå†…å®¹ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # è§£æé¡µé¢
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æˆ¿ä»·å…³é”®è¯çš„é“¾æ¥
            all_links = soup.find_all('a', href=True)
            print(f"é¡µé¢ä¸­æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
            
            page_new_records = []
            found_latest_record = False  # æ˜¯å¦é‡åˆ°CSVä¸­çš„ç¬¬ä¸€æ¡è®°å½•ï¼ˆæœ€æ–°è®°å½•ï¼‰
            
            for link in all_links:
                link_text = link.get_text(strip=True)
                href = link.get('href')
                
                # æ›´å®½æ¾çš„åŒ¹é…æ¡ä»¶
                if any(keyword in link_text for keyword in [
                    "70ä¸ªå¤§ä¸­åŸå¸‚", "å•†å“ä½å®…é”€å”®ä»·æ ¼", "æˆ¿ä»·å˜åŠ¨", "ä½å®…é”€å”®ä»·æ ¼"
                ]):
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('http'):
                        full_url = href
                    elif href.startswith('./'):
                        full_url = urljoin(self.data_url, href[2:])
                    else:
                        full_url = urljoin(self.base_url, href)
                    
                    # æ£€æŸ¥æ˜¯å¦é‡åˆ°CSVä¸­çš„ç¬¬ä¸€æ¡è®°å½•ï¼ˆæœ€æ–°è®°å½•ï¼‰
                    if self.latest_url and full_url == self.latest_url:
                        print(f"é‡åˆ°CSVä¸­çš„æœ€æ–°è®°å½•ï¼Œåœæ­¢ç¿»é¡µ: {link_text}")
                        found_latest_record = True
                        break
                    
                    # å¦‚æœURLå·²å­˜åœ¨ä½†ä¸æ˜¯æœ€æ–°è®°å½•ï¼Œè·³è¿‡ä½†ç»§ç»­ç¿»é¡µ
                    if full_url in self.existing_urls:
                        print(f"é‡åˆ°å·²å­˜åœ¨æ¡ç›®ï¼ˆéæœ€æ–°ï¼‰ï¼Œè·³è¿‡ä½†ç»§ç»­ç¿»é¡µ: {link_text}")
                        continue
                    
                    if full_url in seen_urls:
                        continue
                    
                    seen_urls.add(full_url)
                    
                    # æå–æ—¥æœŸ
                    date_str = self.extract_date_from_link_context(link, soup)
                    
                    record = {
                        'title': link_text,
                        'url': full_url,
                        'date': date_str
                    }
                    page_new_records.append(record)
                    print(f"å‘ç°æ–°è®°å½•: {link_text} -> {full_url}")
            
            # æ·»åŠ æ–°è®°å½•
            all_new_records.extend(page_new_records)
            print(f"ç¬¬ {page_num} é¡µæ‰¾åˆ° {len(page_new_records)} ä¸ªæ–°è®°å½•")
            
            # å¦‚æœé‡åˆ°CSVä¸­çš„ç¬¬ä¸€æ¡è®°å½•ï¼ˆæœ€æ–°è®°å½•ï¼‰ï¼Œåœæ­¢ç¿»é¡µ
            if found_latest_record:
                break
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µURL
            next_url = self.find_next_page_url(soup, current_url)
            if next_url:
                current_url = next_url
                page_num += 1
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            else:
                print(f"æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œåœæ­¢ç¿»é¡µ")
                break
        
        print(f"\nå¢å¼ºç‰ˆæå–å®Œæˆï¼Œå…±æ£€æŸ¥äº† {page_num} é¡µï¼Œæ‰¾åˆ° {len(all_new_records)} ä¸ªæ–°è®°å½•")
        return all_new_records
    
    def extract_date_from_link_context(self, link, soup):
        """ä»é“¾æ¥ä¸Šä¸‹æ–‡æå–æ—¥æœŸ"""
        # å°è¯•ä»çˆ¶å…ƒç´ è·å–æ—¥æœŸ
        parent = link.parent
        while parent and parent.name != 'body':
            parent_text = parent.get_text()
            
            # æŸ¥æ‰¾æ—¥æœŸæ¨¡å¼
            date_patterns = [
                r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2025-10-20
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2025å¹´10æœˆ20æ—¥
                r'(\d{4})/(\d{1,2})/(\d{1,2})',  # 2025/10/20
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, parent_text)
                if match:
                    year, month, day = match.groups()
                    return f"{year}/{int(month)}/{int(day)}"
            
            parent = parent.parent
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä»é“¾æ¥æ–‡æœ¬æ¨æ–­
        date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ', link.get_text())
        if date_match:
            year, month = date_match.groups()
            # æ ¹æ®å†å²æ•°æ®ï¼Œæˆ¿ä»·æ•°æ®é€šå¸¸åœ¨æ¬¡æœˆä¸­æ—¬å‘å¸ƒ
            if int(month) == 12:
                return f"{int(year)+1}/1/15"
            else:
                return f"{year}/{int(month)+1}/15"
        
        # é»˜è®¤ä½¿ç”¨å½“å‰æ—¥æœŸ
        return datetime.now().strftime("%Y/%m/%d")
    
    def get_accurate_date_from_detail_page(self, url):
        """ä»è¯¦æƒ…é¡µé¢è·å–å‡†ç¡®çš„å‘å¸ƒæ—¥æœŸ"""
        try:
            content = self.get_page_content(url)
            if not content:
                return None
                
            soup = BeautifulSoup(content, 'html.parser')
            
            # å¯»æ‰¾å‘å¸ƒæ—¥æœŸçš„å¸¸è§ä½ç½®
            date_selectors = [
                '.artical-info',
                '.article-info', 
                '.publish-time',
                '.time',
                '.date'
            ]
            
            for selector in date_selectors:
                date_element = soup.select_one(selector)
                if date_element:
                    date_text = date_element.get_text()
                    parsed_date = self.parse_date_from_text(date_text)
                    if parsed_date:
                        return parsed_date
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåœ¨æ•´ä¸ªé¡µé¢ä¸­æœç´¢æ—¥æœŸæ¨¡å¼
            page_text = soup.get_text()
            parsed_date = self.parse_date_from_text(page_text)
            if parsed_date:
                return parsed_date
                
        except Exception as e:
            print(f"è·å–è¯¦æƒ…é¡µé¢æ—¥æœŸæ—¶å‡ºé”™: {e}")
        
        return None
    
    def update_csv_file(self, new_records):
        """æ›´æ–°CSVæ–‡ä»¶"""
        if not new_records:
            print("æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ·»åŠ ")
            return
        
        # ä¸ºæ–°è®°å½•è·å–æ›´å‡†ç¡®çš„æ—¥æœŸ
        for record in new_records:
            print(f"è·å–è¯¦ç»†æ—¥æœŸ: {record['title']}")
            accurate_date = self.get_accurate_date_from_detail_page(record['url'])
            if accurate_date:
                record['date'] = accurate_date
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
        new_records.sort(key=lambda x: datetime.strptime(x['date'], "%Y/%m/%d"), reverse=True)
        
        # å‡†å¤‡å†™å…¥CSVçš„æ•°æ®
        all_rows = []
        
        # æ·»åŠ æ ‡é¢˜è¡Œ
        all_rows.append(["æ ‡é¢˜", "æ ‡é¢˜é“¾æ¥", "æ—¶é—´"])
        
        # æ·»åŠ æ–°è®°å½•
        for record in new_records:
            all_rows.append([record['title'], record['url'], record['date']])
        
        # æ·»åŠ ç°æœ‰è®°å½•ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰
        if len(self.existing_data) > 1:
            all_rows.extend(self.existing_data[1:])
        
        # å†™å…¥CSVæ–‡ä»¶
        try:
            with open(self.csv_file_path, 'w', encoding='utf-8', newline='') as file:
                writer = csv.writer(file)
                writer.writerows(all_rows)
            
            print(f"æˆåŠŸæ›´æ–°CSVæ–‡ä»¶ï¼Œæ·»åŠ äº† {len(new_records)} æ¡æ–°è®°å½•")
            for record in new_records:
                print(f"  - {record['title']} ({record['date']})")
                
        except Exception as e:
            print(f"å†™å…¥CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def collect_new_data(self, new_records):
        """ä¸ºæ–°è®°å½•é‡‡é›†æ•°æ®"""
        if not new_records:
            return True
            
        print("\n" + "=" * 50)
        print("å¼€å§‹é‡‡é›†æ–°æ•°æ®...")
        print("=" * 50)
        
        try:
            # å¯¼å…¥æ•°æ®é‡‡é›†å™¨
            from data_collector import HousePriceDataCollector
            collector = HousePriceDataCollector()
            
            success_count = 0
            total_count = len(new_records)
            
            for i, record in enumerate(new_records, 1):
                print(f"\næ­£åœ¨é‡‡é›†ç¬¬ {i}/{total_count} ä¸ªæ•°æ®:")
                print(f"æ ‡é¢˜: {record['title']}")
                print(f"URL: {record['url']}")
                print(f"æ—¥æœŸ: {record['date']}")
                
                # é‡‡é›†å•ä¸ªURLçš„æ•°æ®
                success = collector.collect_single_url_data(
                    record['url'], 
                    record['title'], 
                    record['date']
                )
                
                if success:
                    success_count += 1
                    print(f"âœ… é‡‡é›†æˆåŠŸ ({success_count}/{total_count})")
                else:
                    print(f"âŒ é‡‡é›†å¤±è´¥")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if i < total_count:
                    time.sleep(2)
            
            print(f"\næ•°æ®é‡‡é›†å®Œæˆ: æˆåŠŸ {success_count}/{total_count}")
            return success_count > 0
            
        except ImportError as e:
            print(f"æ— æ³•å¯¼å…¥æ•°æ®é‡‡é›†å™¨: {e}")
            return False
        except Exception as e:
            print(f"æ•°æ®é‡‡é›†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return False
    
    def process_data_to_json(self):
        """å¤„ç†XMLæ•°æ®ä¸ºJSONæ ¼å¼"""
        print("\n" + "=" * 50)
        print("å¼€å§‹å¤„ç†æ•°æ®ä¸ºJSONæ ¼å¼...")
        print("=" * 50)
        
        try:
            # å¯¼å…¥æ‰¹é‡å¤„ç†å™¨
            from batch_process_all_cities import BatchProcessor
            
            # åˆ›å»ºæ‰¹é‡å¤„ç†å™¨å¹¶å¼€å§‹å¤„ç†
            processor = BatchProcessor()
            processor.process_all()
            
            print("âœ… æ•°æ®å¤„ç†å®Œæˆï¼JSONæ–‡ä»¶å·²æ›´æ–°ã€‚")
            return True
            
        except ImportError as e:
            print(f"æ— æ³•å¯¼å…¥æ‰¹é‡å¤„ç†å™¨: {e}")
            return False
        except Exception as e:
            print(f"æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return False
    
    def run(self, auto_collect=False):
        """
        è¿è¡Œæ›´æ–°ç¨‹åº
        
        Args:
            auto_collect (bool): æ˜¯å¦è‡ªåŠ¨æ‰§è¡Œæ•°æ®é‡‡é›†å’Œå¤„ç†ï¼Œé»˜è®¤ä¸ºFalse
        """
        print("å¼€å§‹æ›´æ–°æˆ¿ä»·æ•°æ®URL...")
        print("=" * 50)
        
        # 1. è¯»å–ç°æœ‰æ•°æ®
        self.load_existing_data()
        
        # 2. é¦–å…ˆå°è¯•å¢å¼ºç‰ˆæ•°æ®æå–
        print("å°è¯•å¢å¼ºç‰ˆæ•°æ®æå–...")
        new_records = self.extract_house_price_data_enhanced()
        
        # 3. å¦‚æœå¢å¼ºç‰ˆæ²¡æœ‰æ‰¾åˆ°æ–°æ•°æ®ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
        if not new_records:
            print("å¢å¼ºç‰ˆæœªæ‰¾åˆ°æ–°æ•°æ®ï¼Œå°è¯•åŸå§‹ç¿»é¡µæ–¹æ³•...")
            new_records = self.extract_house_price_data()
        
        # 4. æ›´æ–°CSVæ–‡ä»¶
        self.update_csv_file(new_records)
        
        # 5. æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦è‡ªåŠ¨æ‰§è¡Œæ•°æ®é‡‡é›†å’Œå¤„ç†
        if new_records and auto_collect:
            print(f"\nå‘ç° {len(new_records)} ä¸ªæ–°è®°å½•ï¼Œå¼€å§‹è‡ªåŠ¨å¤„ç†...")
            
            # 5.1 é‡‡é›†æ–°æ•°æ®
            collect_success = self.collect_new_data(new_records)
            
            if collect_success:
                # 5.2 å¤„ç†æ•°æ®ä¸ºJSONæ ¼å¼
                process_success = self.process_data_to_json()
                
                if process_success:
                    print("\nğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸï¼")
                    print("- âœ… URLæ›´æ–°å®Œæˆ")
                    print("- âœ… æ•°æ®é‡‡é›†å®Œæˆ") 
                    print("- âœ… JSONæ–‡ä»¶æ›´æ–°å®Œæˆ")
                else:
                    print("\nâš ï¸  éƒ¨åˆ†æµç¨‹å®Œæˆ:")
                    print("- âœ… URLæ›´æ–°å®Œæˆ")
                    print("- âœ… æ•°æ®é‡‡é›†å®Œæˆ")
                    print("- âŒ JSONå¤„ç†å¤±è´¥")
            else:
                print("\nâš ï¸  æ•°æ®é‡‡é›†å¤±è´¥ï¼Œè·³è¿‡JSONå¤„ç†")
        elif new_records:
            print(f"\nâœ… å‘ç° {len(new_records)} ä¸ªæ–°è®°å½•ï¼Œå·²æ›´æ–°åˆ°CSVæ–‡ä»¶")
            print("æç¤º: ä½¿ç”¨ --auto-collect å‚æ•°å¯è‡ªåŠ¨æ‰§è¡Œæ•°æ®é‡‡é›†")
        else:
            print("\nâœ… æ— æ–°æ•°æ®ï¼Œæµç¨‹å®Œæˆ")
        
        print("=" * 50)
        print("æ›´æ–°å®Œæˆ!")

def main():
    import sys
    # æ£€æŸ¥æ˜¯å¦æœ‰ --auto-collect å‚æ•°
    auto_collect = '--auto-collect' in sys.argv or '-a' in sys.argv
    
    updater = HousePriceURLUpdater()
    updater.run(auto_collect=auto_collect)

if __name__ == "__main__":
    main()
